{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "EF_sent = [i.replace('\\n','')for i in open('EF666.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get have VT_tag sentence\n",
    "Tmp_VT = []\n",
    "for i in EF_sent:\n",
    "    if 'VT+' in i or 'VT-' in i:\n",
    "        Tmp_VT.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I arrived there , I saw that someone {+had//VT+} forced the lock of the door .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tmp_VT[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2before_after_taglook(text , tag):\n",
    "    before, after , tmp_V = [], [] ,[]\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+'):\n",
    "            after_word = token[2:-2].split('//')[0].replace('\\u3000',' ')\n",
    "            t = token[2:-2].split('//')[1]\n",
    "            if t==tag:\n",
    "                after.append(token)\n",
    "                tmp_V.append(token)\n",
    "            else:\n",
    "                after.append(after_word)\n",
    "        elif token.startswith('[-'):\n",
    "            if token.endswith('+}'):\n",
    "                delete_tmp, insert_tmp = token[2:-2].split('-]{+')\n",
    "                delete = delete_tmp.split('//')[0].replace('\\u3000',' ')\n",
    "                insert = insert_tmp.split('//')[0].replace('\\u3000',' ')\n",
    "                t = insert_tmp.split('//')[1]\n",
    "                if t==tag:\n",
    "                    before.append(token)\n",
    "                    after.append(token)\n",
    "                    tmp_V.append((token))\n",
    "                else:\n",
    "                    before.append(delete)\n",
    "                    after.append(insert)\n",
    "            else:\n",
    "                before_word = token[2:-2].split('//')[0].replace('\\u3000',' ')\n",
    "                t = token[2:-2].split('//')[1]\n",
    "                if t==tag:\n",
    "                    before.append(token)\n",
    "                    tmp_V.append(token)\n",
    "                else:\n",
    "                    before.append(before_word)\n",
    "        else:\n",
    "            toke = token.replace('\\u3000',' ')\n",
    "            before.append(token)\n",
    "            after.append(token)\n",
    "    return  ' '.join(after) , tmp_V\n",
    "def diff_after_before(text):\n",
    "    before, after = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+'):\n",
    "            after_word = token[2:-2].split('//')[0]\n",
    "            after.append(after_word)\n",
    "        elif token.startswith('[-'):\n",
    "            if token.endswith('+}'):\n",
    "                delete_tmp, insert_tmp = token[2:-2].split('-]{+')\n",
    "                delete = delete_tmp.split('//')[0]\n",
    "                insert = insert_tmp.split('//')[0]\n",
    "                before.append(delete)\n",
    "                after.append(insert)\n",
    "            else:\n",
    "                before_word = token[2:-2].split('//')[0]\n",
    "                before.append(before_word)\n",
    "        else:\n",
    "            before.append(token)\n",
    "            after.append(token)\n",
    "    return ' '.join(before).replace('\\u3000',' '), ' '.join(after).replace('\\u3000',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seg_index(token):\n",
    "    l = len(token)\n",
    "    seg_index_tmp=[]\n",
    "    for index, word in enumerate(token):\n",
    "        if 'VT' in word:\n",
    "            seg_index_tmp.append((index-1,index,index+1))\n",
    "    seg_ngram_index = []\n",
    "    for ii in seg_index_tmp:\n",
    "        seg_index = []\n",
    "        for i in ii:\n",
    "            if i<0:\n",
    "                seg_index.append(0)\n",
    "            elif i>=l:\n",
    "                seg_index.append(l-1)\n",
    "            else:\n",
    "                seg_index.append(i)\n",
    "        seg_index = sorted(set(seg_index))\n",
    "        seg_ngram_index.append(seg_index)\n",
    "    return seg_ngram_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segment(tag_sen , tag_arry):\n",
    "    token = tag_sen.split(' ')\n",
    "    index_arry = find_seg_index(token)\n",
    "    segment_arry = []\n",
    "    for index , tag in zip(index_arry , tag_arry):\n",
    "        seg = ' '.join([token[i] for i in index])\n",
    "        before , after = diff_after_before(seg)\n",
    "        segment_arry.append((before , after))\n",
    "    return segment_arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segment_position(full, part):\n",
    "    full = full.split()\n",
    "    part = part.split()\n",
    "    length = len(part)\n",
    "    for i, sublist in enumerate((full[i:i+length] for i in range(len(full)))):\n",
    "        if part == sublist:\n",
    "            return [i+c for c in range(length)]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_index_match(sen , index):\n",
    "    all_tag = ['VBP' , 'VBZ' , 'VBG' , 'VBN' , 'VBD' , 'VB']\n",
    "    word = ['will']\n",
    "    doc = nlp(sen)\n",
    "    show = []\n",
    "    for i in index:\n",
    "        if doc[i].text in word or doc[i].tag_ in all_tag:\n",
    "            show.append((doc[i].text , doc[i].tag_))\n",
    "    return show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern_spacy_result(sen ,pattern='VT'):\n",
    "    tag_sen , tag = diff2before_after_taglook(sen , pattern)\n",
    "    before , after = diff_after_before(tag_sen)\n",
    "    before  = ' '.join([i for i in before.split(' ') if i])\n",
    "    after = ' '.join([i for i in after.split(' ') if i])\n",
    "    segment_arry = find_segment(tag_sen , tag)\n",
    "    result = []\n",
    "    for segment in segment_arry:\n",
    "        before_seg = segment[0]\n",
    "        after_seg = segment[1]\n",
    "        b_index = find_segment_position(before , before_seg)\n",
    "        a_index = find_segment_position(after , after_seg)\n",
    "        b_spacy_result = spacy_index_match( before , b_index)\n",
    "        a_spacy_result = spacy_index_match( after , a_index)\n",
    "        result.append((b_spacy_result , a_spacy_result))\n",
    "        #print(b_spacy_result , a_spacy_result)\n",
    "    return result , tag_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(pair):\n",
    "    try:\n",
    "        if not future(pair):\n",
    "            if pair[0][1]=='VBP' or pair[0][1]=='VBZ':\n",
    "                return 'now'\n",
    "    except:pass\n",
    "def past(pair):\n",
    "    try:\n",
    "        if not future(pair):\n",
    "            if pair[0][1]=='VBD':\n",
    "                return 'past'\n",
    "    except:pass\n",
    "def future(pair):\n",
    "    try:\n",
    "        if ((pair[0][0]=='is' or pair[0][0]=='are' or pair[0][0]=='am')  and pair[1][0]=='going') or pair[0][0]=='will':\n",
    "            if pair[-1][1]=='VB':\n",
    "                return 'future'\n",
    "    except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple(pair):\n",
    "    try:\n",
    "        if not nowing(pair) and not finish(pair) and not finishing(pair):\n",
    "            if now(pair) or past(pair) or future(pair):\n",
    "                return 'simple'\n",
    "    except:pass\n",
    "def nowing(pair):\n",
    "    try:\n",
    "        if len(pair)==2:\n",
    "            if pair[0][0]=='is' or pair[0][0]=='are' or pair[0][0]=='am' or pair[0][0]=='was' or pair[0][0]=='were':\n",
    "                if pair[-1][1]=='VBG':\n",
    "                    return 'nowing'\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='be':\n",
    "                    if pair[-1][1]=='VBG':\n",
    "                        return 'nowing'\n",
    "    except:pass\n",
    "def finish(pair):\n",
    "    try:\n",
    "        if len(pair)==2:\n",
    "            if pair[0][0]=='have' or pair[0][0]=='has'or pair[0][0]=='had':\n",
    "                if pair[-1][1]=='VBN' and pair[-1][0] != 'been':\n",
    "                    return 'finish'\n",
    "        if len(pair)==3:\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='have':\n",
    "                    if pair[-1][1]=='VBN' and pair[-1][0] != 'been':\n",
    "                        return 'finish'\n",
    "    except:pass\n",
    "def finishing(pair):\n",
    "    try:\n",
    "        if len(pair)==3:\n",
    "            if pair[0][0]=='have' or pair[0][0]=='has'or pair[0][0]=='had':\n",
    "                if pair[1][0]=='been':\n",
    "                    if pair[-1][1]=='VBG':\n",
    "                        return 'finishing'\n",
    "        if len(pair)==4:\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='have':\n",
    "                    if pair[2][0]=='been':\n",
    "                        if pair[-1][1]=='VBG':\n",
    "                            return 'finishing'\n",
    "    except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_time(tmp , now , past , future):\n",
    "    if not tmp:\n",
    "        if now : tmp = now\n",
    "        elif past : tmp = past\n",
    "        elif future : tmp = future\n",
    "    return tmp\n",
    "def find_tense(tmp , simple , nowing , finish , finishing):\n",
    "    if not tmp:\n",
    "        if simple : tmp = simple\n",
    "        elif nowing : tmp = nowing\n",
    "        elif finish : tmp= finish\n",
    "        elif finishing : tmp = finishing\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(spacy_result , tag_sen):\n",
    "    Rule = []\n",
    "    for pair in spacy_result:\n",
    "        if pair[0] and pair[1]:\n",
    "            before , after = pair[0] , pair[1]\n",
    "            tmp_b , tmp_bb , tmp_a , tmp_aa  = '' , '' , '' , ''\n",
    "            now_b , past_b , future_b = now(before) , past(before) , future(before)\n",
    "            now_a , past_a , future_a = now(after) , past(after) , future(after)\n",
    "            #print(now_b , past_b , future_b)\n",
    "            #print(now_a , past_a , future_a)\n",
    "            simple_b , nowing_b , finish_b , finishing_b = simple(before) , nowing(before) , finish(before) , finishing(before)\n",
    "            simple_a , nowing_a , finish_a , finishing_a = simple(after) , nowing(after) , finish(after) , finishing(after)\n",
    "            tmp_b = find_time(tmp_b , now_b , past_b , future_b)\n",
    "            tmp_bb = find_tense(tmp_bb , simple_b , nowing_b , finish_b , finishing_b )\n",
    "            tmp_a = find_time(tmp_a , now_a , past_a , future_a)\n",
    "            tmp_aa = find_tense(tmp_aa , simple_a , nowing_a , finish_a , finishing_a )\n",
    "            #print(tmp_b , tmp_a)\n",
    "            #print(tmp_bb , tmp_aa)\n",
    "            if tmp_a and tmp_b:\n",
    "                rule1 = tmp_b + '->' + tmp_a\n",
    "                Rule.append((rule1 , pair))\n",
    "                #print(rule)\n",
    "                #print(before , after)\n",
    "                #print(tag_sen)\n",
    "            if tmp_aa and tmp_bb:\n",
    "                rule2 = tmp_bb + '->' + tmp_aa\n",
    "                Rule.append((rule2 , pair))\n",
    "                #print(before , after)\n",
    "                #print(tag_sen)\n",
    "    return Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "def pair_key(pair):\n",
    "    key_verb =''\n",
    "    before_word = [lemmatizer(i[0] , 'VERB')[0] for i in pair[0]]\n",
    "    after_word = [lemmatizer(i[0] , 'VERB')[0] for i in pair[1]]\n",
    "    #print(before_word , after_word)\n",
    "    #list3 = set(before_word)&set(after_word)\n",
    "    #list4 = sorted(list3, key = lambda k : before_word.index(k))\n",
    "    return after_word[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(lambda: defaultdict(lambda: []))\n",
    "for index , sentence  in enumerate(Tmp_VT[:]):\n",
    "    try:\n",
    "        result , tag_sen = find_pattern_spacy_result(sentence)\n",
    "        for item in match(result , tag_sen):\n",
    "            rule , pair = item[0] , item[1]\n",
    "            key = pair_key(pair)\n",
    "            d[rule][key].append(index)\n",
    "    except:print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_index_result(index):\n",
    "    result , tag_sen = find_pattern_spacy_result(Tmp_VT[index])\n",
    "    for item in match(result , tag_sen):\n",
    "        rule , pair = item[0] , item[1]\n",
    "        key = pair_key(pair)\n",
    "        print('Rule : ',rule ,\"|\", 'Main verb : ',key ,\"|\", pair )  \n",
    "    print(\"# Sentence : #\")\n",
    "    print('##################################################')\n",
    "    print(tag_sen)\n",
    "    print('##################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'simple->nowing'\n",
    "# 'now->now'\n",
    "# 'simple->simple'\n",
    "# 'now->future'\n",
    "# 'future->now'\n",
    "# 'finish->finish'\n",
    "# 'nowing->nowing'\n",
    "# 'future->future'\n",
    "# 'simple->finish'\n",
    "# 'finishing->finish'\n",
    "# 'nowing->simple'\n",
    "# 'simple->finishing'\n",
    "# 'nowing->finishing'\n",
    "# 'finishing->finishing'\n",
    "# 'finish->finishing'\n",
    "# 'finish->simple'\n",
    "# 'nowing->finish'\n",
    "# 'finishing->simple'\n",
    "# 'finishing->nowing'\n",
    "# 'finish->nowing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_all_rule_count(d):\n",
    "    def look_rule_count(d , rule):\n",
    "        count = 0\n",
    "        for verb in d[rule].keys():\n",
    "            count+=len(d[rule][verb])\n",
    "        return count\n",
    "    for rule in d.keys():\n",
    "        print(rule , look_rule_count(d , rule))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_rule_verb_count(d , rule , number=None):\n",
    "    dd = {}\n",
    "    for verb in d[rule].keys():\n",
    "        count = 0\n",
    "        count+=len(d[rule][verb])\n",
    "        dd[verb] = count\n",
    "    print(sorted(dd.items() , key = lambda x:x[1] , reverse=True)[:number])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look_all_rule_count(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 2501), ('have', 1081), ('work', 501), ('come', 305), ('get', 278), ('live', 273), ('like', 255), ('make', 250), ('do', 246), ('love', 224)]\n"
     ]
    }
   ],
   "source": [
    "look_rule_verb_count(d , 'past->now' , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confuse_look(d , rule , verb , number=10):\n",
    "    print(\"# Rule #\")\n",
    "    for i in d[rule][verb][:number]:\n",
    "        look_index_result(i)\n",
    "        print('-------------------------END--------------------------------------------')\n",
    "    print('')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rule #\n",
      "Rule :  past->now | Main verb :  be | ([('were', 'VBD')], [('are', 'VBP')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('were', 'VBD')], [('are', 'VBP')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "Plot : They [-were//VT-]{+are//VT+} in the city when vampires attack \" The Twilight \" is a book that captures the romance of a couple , a young vampire and a mortal girl , who struggle to get all this love to live .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "My name is Elaine , I 'm twenty - five years old , I 'm from Taiwan but I live in China , Guangzhou . There are seven people in my family : my father , my mother , my brother , my husband and I have three daughters . My father , my mother , my brother , they all live in Taiwan , My brother is very young . He [-was//VT-]{+is//VT+} only twenty - seven years old but my husband is thirty - two . How about you ?\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "This self - help book is very motivating , but is it really possible to change your current job , even though you could n't wish for anything better in your life ? I think it [-was//VT-]{+is//VT+} not so easy , otherwise everyone would be happy in their career , and nobody would be suffering from anxiety or would be under stress at work .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "If the weather [-was　n't//VT-]{+is　n't//VT+} rainy , you should walk the dogs twice a day .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('wanted', 'VBD'), ('is', 'VBZ')], [('want', 'VBP'), ('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('wanted', 'VBD'), ('is', 'VBZ')], [('want', 'VBP'), ('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "Sometimes what we really [-wanted//VT-]{+want//VT+} is a stable job and life .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "My next meeting [-was//VT-]{+is//VT+} in London at 9 am on Wednesday , June 3rd .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  past->now | Main verb :  have | ([('had', 'VBD')], [('have', 'VBP')])\n",
      "Rule :  simple->simple | Main verb :  have | ([('had', 'VBD')], [('have', 'VBP')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "there [-was//VT-]{+is//VT+} a presentation at The next day I [-had//VT-]{+have//VT+} a meeting with the marketing team in UK .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "It [-was//VT-]{+is//VT+} very beautiful .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  past->now | Main verb :  have | ([('had', 'VBD')], [('has', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  have | ([('had', 'VBD')], [('has', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "seldom watch TV , because our son [-was//VT-]{+is//VT+} in middle school , he [-had//VT-]{+has//VT+} much home work to do .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  past->now | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  be | ([('was', 'VBD')], [('is', 'VBZ')])\n",
      "Rule :  past->now | Main verb :  have | ([('had', 'VBD')], [('has', 'VBZ')])\n",
      "Rule :  simple->simple | Main verb :  have | ([('had', 'VBD')], [('has', 'VBZ')])\n",
      "# Sentence : #\n",
      "##################################################\n",
      "The design [-was//VT-]{+is//VT+} C - shaped , inside [-was//VT-]{+is//VT+} a person with a square face , holding both arms up towards the sky ; it [-had//VT-]{+has//VT+} a long , narrow and rectangular body .\n",
      "##################################################\n",
      "-------------------------END--------------------------------------------\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "confuse_look(d , 'past->now' , 'be' , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill as pickle\n",
    "#with open('model.pkl', 'wb') as file:\n",
    "#    pickle.dump(d, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model.pkl', 'rb') as in_strm:\n",
    "#    datastruct = dill.load(in_strm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmoblistm",
   "language": "python",
   "name": "elmoblistm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
