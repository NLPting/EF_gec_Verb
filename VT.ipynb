{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "EF_sent = [i.replace('\\n','')for i in open('EF666.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get have VT_tag sentence\n",
    "Tmp_VT = []\n",
    "for i in EF_sent:\n",
    "    if 'VT+' in i or 'VT-' in i:\n",
    "        Tmp_VT.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2before_after_taglook(text , tag):\n",
    "    before, after , tmp_V = [], [] ,[]\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+'):\n",
    "            after_word = token[2:-2].split('//')[0].replace('\\u3000',' ')\n",
    "            t = token[2:-2].split('//')[1]\n",
    "            if t==tag:\n",
    "                after.append(token)\n",
    "                tmp_V.append(token)\n",
    "            else:\n",
    "                after.append(after_word)\n",
    "        elif token.startswith('[-'):\n",
    "            if token.endswith('+}'):\n",
    "                delete_tmp, insert_tmp = token[2:-2].split('-]{+')\n",
    "                delete = delete_tmp.split('//')[0].replace('\\u3000',' ')\n",
    "                insert = insert_tmp.split('//')[0].replace('\\u3000',' ')\n",
    "                t = insert_tmp.split('//')[1]\n",
    "                if t==tag:\n",
    "                    before.append(token)\n",
    "                    after.append(token)\n",
    "                    tmp_V.append((token))\n",
    "                else:\n",
    "                    before.append(delete)\n",
    "                    after.append(insert)\n",
    "            else:\n",
    "                before_word = token[2:-2].split('//')[0].replace('\\u3000',' ')\n",
    "                t = token[2:-2].split('//')[1]\n",
    "                if t==tag:\n",
    "                    before.append(token)\n",
    "                    tmp_V.append(token)\n",
    "                else:\n",
    "                    before.append(before_word)\n",
    "        else:\n",
    "            toke = token.replace('\\u3000',' ')\n",
    "            before.append(token)\n",
    "            after.append(token)\n",
    "    return  ' '.join(after) , tmp_V\n",
    "def diff_after_before(text):\n",
    "    before, after = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+'):\n",
    "            after_word = token[2:-2].split('//')[0]\n",
    "            after.append(after_word)\n",
    "        elif token.startswith('[-'):\n",
    "            if token.endswith('+}'):\n",
    "                delete_tmp, insert_tmp = token[2:-2].split('-]{+')\n",
    "                delete = delete_tmp.split('//')[0]\n",
    "                insert = insert_tmp.split('//')[0]\n",
    "                before.append(delete)\n",
    "                after.append(insert)\n",
    "            else:\n",
    "                before_word = token[2:-2].split('//')[0]\n",
    "                before.append(before_word)\n",
    "        else:\n",
    "            before.append(token)\n",
    "            after.append(token)\n",
    "    return ' '.join(before).replace('\\u3000',' '), ' '.join(after).replace('\\u3000',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seg_index(token):\n",
    "    l = len(token)\n",
    "    seg_index_tmp=[]\n",
    "    for index, word in enumerate(token):\n",
    "        if 'VT' in word:\n",
    "            seg_index_tmp.append((index-1,index,index+1))\n",
    "    seg_ngram_index = []\n",
    "    for ii in seg_index_tmp:\n",
    "        seg_index = []\n",
    "        for i in ii:\n",
    "            if i<0:\n",
    "                seg_index.append(0)\n",
    "            elif i>=l:\n",
    "                seg_index.append(l-1)\n",
    "            else:\n",
    "                seg_index.append(i)\n",
    "        seg_index = sorted(set(seg_index))\n",
    "        seg_ngram_index.append(seg_index)\n",
    "    return seg_ngram_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segment(tag_sen , tag_arry):\n",
    "    token = tag_sen.split(' ')\n",
    "    index_arry = find_seg_index(token)\n",
    "    segment_arry = []\n",
    "    for index , tag in zip(index_arry , tag_arry):\n",
    "        seg = ' '.join([token[i] for i in index])\n",
    "        before , after = diff_after_before(seg)\n",
    "        segment_arry.append((before , after))\n",
    "    return segment_arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segment_position(full, part):\n",
    "    full = full.split()\n",
    "    part = part.split()\n",
    "    length = len(part)\n",
    "    for i, sublist in enumerate((full[i:i+length] for i in range(len(full)))):\n",
    "        if part == sublist:\n",
    "            return [i+c for c in range(length)]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_index_match(sen , index):\n",
    "    all_tag = ['VBP' , 'VBZ' , 'VBG' , 'VBN' , 'VBD' , 'VB']\n",
    "    word = ['will']\n",
    "    doc = nlp(sen)\n",
    "    show = []\n",
    "    for i in index:\n",
    "        if doc[i].text in word or doc[i].tag_ in all_tag:\n",
    "            show.append((doc[i].text , doc[i].tag_))\n",
    "    return show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern_spacy_result(sen ,pattern='VT'):\n",
    "    tag_sen , tag = diff2before_after_taglook(sen , pattern)\n",
    "    before , after = diff_after_before(tag_sen)\n",
    "    before  = ' '.join([i for i in before.split(' ') if i])\n",
    "    after = ' '.join([i for i in after.split(' ') if i])\n",
    "    segment_arry = find_segment(tag_sen , tag)\n",
    "    result = []\n",
    "    for segment in segment_arry:\n",
    "        before_seg = segment[0]\n",
    "        after_seg = segment[1]\n",
    "        b_index = find_segment_position(before , before_seg)\n",
    "        a_index = find_segment_position(after , after_seg)\n",
    "        b_spacy_result = spacy_index_match( before , b_index)\n",
    "        a_spacy_result = spacy_index_match( after , a_index)\n",
    "        result.append((b_spacy_result , a_spacy_result))\n",
    "        #print(b_spacy_result , a_spacy_result)\n",
    "    return result , tag_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(pair):\n",
    "    try:\n",
    "        if not future(pair):\n",
    "            if pair[0][1]=='VBP' or pair[0][1]=='VBZ':\n",
    "                return 'now'\n",
    "    except:pass\n",
    "def past(pair):\n",
    "    try:\n",
    "        if not future(pair):\n",
    "            if pair[0][1]=='VBD':\n",
    "                return 'past'\n",
    "    except:pass\n",
    "def future(pair):\n",
    "    try:\n",
    "        if ((pair[0][0]=='is' or pair[0][0]=='are' or pair[0][0]=='am')  and pair[1][0]=='going') or pair[0][0]=='will':\n",
    "            if pair[-1][1]=='VB':\n",
    "                return 'future'\n",
    "    except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple(pair):\n",
    "    try:\n",
    "        if not nowing(pair) and not finish(pair) and not finishing(pair):\n",
    "            if now(pair) or past(pair) or future(pair):\n",
    "                return 'simple'\n",
    "    except:pass\n",
    "def nowing(pair):\n",
    "    try:\n",
    "        if len(pair)==2:\n",
    "            if pair[0][0]=='is' or pair[0][0]=='are' or pair[0][0]=='am' or pair[0][0]=='was' or pair[0][0]=='were':\n",
    "                if pair[-1][1]=='VBG':\n",
    "                    return 'nowing'\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='be':\n",
    "                    if pair[-1][1]=='VBG':\n",
    "                        return 'nowing'\n",
    "    except:pass\n",
    "def finish(pair):\n",
    "    try:\n",
    "        if len(pair)==2:\n",
    "            if pair[0][0]=='have' or pair[0][0]=='has'or pair[0][0]=='had':\n",
    "                if pair[-1][1]=='VBN' and pair[-1][0] != 'been':\n",
    "                    return 'finish'\n",
    "        if len(pair)==3:\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='have':\n",
    "                    if pair[-1][1]=='VBN' and pair[-1][0] != 'been':\n",
    "                        return 'finish'\n",
    "    except:pass\n",
    "def finishing(pair):\n",
    "    try:\n",
    "        if len(pair)==3:\n",
    "            if pair[0][0]=='have' or pair[0][0]=='has'or pair[0][0]=='had':\n",
    "                if pair[1][0]=='been':\n",
    "                    if pair[-1][1]=='VBG':\n",
    "                        return 'finishing'\n",
    "        if len(pair)==4:\n",
    "            if pair[0][0]=='will':\n",
    "                if pair[1][0]=='have':\n",
    "                    if pair[2][0]=='been':\n",
    "                        if pair[-1][1]=='VBG':\n",
    "                            return 'finishing'\n",
    "    except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_time(tmp , now , past , future):\n",
    "    if not tmp:\n",
    "        if now : tmp = now\n",
    "        elif past : tmp = past\n",
    "        elif future : tmp = future\n",
    "    return tmp\n",
    "def find_tense(tmp , simple , nowing , finish , finishing):\n",
    "    if not tmp:\n",
    "        if simple : tmp = simple\n",
    "        elif nowing : tmp = nowing\n",
    "        elif finish : tmp= finish\n",
    "        elif finishing : tmp = finishing\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(spacy_result , tag_sen):\n",
    "    Rule = []\n",
    "    for pair in spacy_result:\n",
    "        if pair[0] and pair[1]:\n",
    "            before , after = pair[0] , pair[1]\n",
    "            tmp_b , tmp_bb , tmp_a , tmp_aa  = '' , '' , '' , ''\n",
    "            now_b , past_b , future_b = now(before) , past(before) , future(before)\n",
    "            now_a , past_a , future_a = now(after) , past(after) , future(after)\n",
    "            #print(now_b , past_b , future_b)\n",
    "            #print(now_a , past_a , future_a)\n",
    "            simple_b , nowing_b , finish_b , finishing_b = simple(before) , nowing(before) , finish(before) , finishing(before)\n",
    "            simple_a , nowing_a , finish_a , finishing_a = simple(after) , nowing(after) , finish(after) , finishing(after)\n",
    "            tmp_b = find_time(tmp_b , now_b , past_b , future_b)\n",
    "            tmp_bb = find_tense(tmp_bb , simple_b , nowing_b , finish_b , finishing_b )\n",
    "            tmp_a = find_time(tmp_a , now_a , past_a , future_a)\n",
    "            tmp_aa = find_tense(tmp_aa , simple_a , nowing_a , finish_a , finishing_a )\n",
    "            #print(tmp_b , tmp_a)\n",
    "            #print(tmp_bb , tmp_aa)\n",
    "            if tmp_a and tmp_b:\n",
    "                rule1 = tmp_b + '->' + tmp_a\n",
    "                Rule.append((rule1 , pair))\n",
    "                #print(rule)\n",
    "                #print(before , after)\n",
    "                #print(tag_sen)\n",
    "            if tmp_aa and tmp_bb:\n",
    "                rule2 = tmp_bb + '->' + tmp_aa\n",
    "                Rule.append((rule2 , pair))\n",
    "                #print(before , after)\n",
    "                #print(tag_sen)\n",
    "    return Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "def pair_key(pair):\n",
    "    key_verb =''\n",
    "    before_word = [lemmatizer(i[0] , 'VERB')[0] for i in pair[0]]\n",
    "    after_word = [lemmatizer(i[0] , 'VERB')[0] for i in pair[1]]\n",
    "    #print(before_word , after_word)\n",
    "    #list3 = set(before_word)&set(after_word)\n",
    "    #list4 = sorted(list3, key = lambda k : before_word.index(k))\n",
    "    return after_word[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(lambda: defaultdict(lambda: []))\n",
    "for index , sentence  in enumerate(Tmp_VT[:]):\n",
    "    try:\n",
    "        result , tag_sen = find_pattern_spacy_result(sentence)\n",
    "        for item in match(result , tag_sen):\n",
    "            rule , pair = item[0] , item[1]\n",
    "            key = pair_key(pair)\n",
    "            d[rule][key].append(index)\n",
    "    except:print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_index_result(index):\n",
    "    result , tag_sen = find_pattern_spacy_result(Tmp_VT[index])\n",
    "    for item in match(result , tag_sen):\n",
    "        rule , pair = item[0] , item[1]\n",
    "        key = pair_key(pair)\n",
    "        print('Rule : ',rule ,\"|\", 'Main verb : ',key ,\"|\", pair )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule :  past->now | Main verb :  work | ([('worked', 'VBD')], [('work', 'VBP')])\n",
      "Rule :  simple->simple | Main verb :  work | ([('worked', 'VBD')], [('work', 'VBP')])\n"
     ]
    }
   ],
   "source": [
    "look_index_result(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill as pickle\n",
    "#with open('model.pkl', 'wb') as file:\n",
    "#    pickle.dump(d, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmoblistm",
   "language": "python",
   "name": "elmoblistm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
